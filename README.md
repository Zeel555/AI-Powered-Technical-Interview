# ğŸš€ AI-Powered-Technical-Interview

An AI-driven full-stack web application designed to simulate real-world technical interviews.  
The system dynamically generates interview questions and evaluates user responses using a locally hosted Large Language Model (LLM).

This project demonstrates the integration of modern AI systems within a scalable MERN-based architecture.

---

## ğŸ¯ Project Objective

Technical interviews require both conceptual clarity and problem-solving ability.  
This platform provides an interactive environment where users can:

- Practice coding interviews
- Attempt conceptual questions
- Submit voice-based answers
- Receive structured AI feedback
- Track performance analytics over time

The application is built using a microservice-inspired architecture to isolate AI computation from the core backend.

---

## ğŸ—ï¸ System Architecture

The system is divided into three major layers:

### 1ï¸âƒ£ Frontend (React + Vite)

Responsible for:
- Interview UI
- Code editor integration (Monaco)
- Audio recording
- Performance visualization
- Authentication handling

The frontend communicates with the Node.js backend via REST APIs.

---

### 2ï¸âƒ£ Backend API (Node.js + Express)

Acts as the **API Gateway**.

Responsibilities:
- User authentication (JWT)
- Password hashing (bcrypt)
- Session storage in MongoDB
- Request validation
- Forwarding AI-related requests to the Python microservice

The backend does NOT perform heavy AI computation.

---

### 3ï¸âƒ£ AI Microservice (FastAPI - Python)

Handles all AI-related operations:

- Dynamic question generation
- Evaluation of coding answers
- Analysis of conceptual responses
- Speech-to-text processing
- Interaction with the local LLM

This separation improves scalability and maintainability.

---

### 4ï¸âƒ£ Local LLM Engine (Ollama - Mistral Model)

The system uses Ollama to run the Mistral model locally.

Why local LLM?

- No external API cost
- Offline capability
- Data privacy
- Full control over model usage

The AI microservice communicates with Ollama through REST calls.

---

## ğŸ”„ Data Flow Overview

User â†’ React Frontend  
â¬‡  
Node.js Backend (Authentication + DB)  
â¬‡  
Python FastAPI Service  
â¬‡  
Ollama (Mistral LLM)  
â¬†  
AI Response â†’ Stored in MongoDB â†’ Displayed in UI  

---

## âœ¨ Core Features

### ğŸ”¹ Role-Based Interview Configuration
Users can select:
- Domain (MERN, Python, Data Science, etc.)
- Difficulty level
- Interview type (Coding / Conceptual / Mixed)

---

### ğŸ”¹ Coding Interview Mode
- Integrated Monaco Editor
- Users write and submit code
- AI evaluates logic and structure
- Generates detailed feedback

---

### ğŸ”¹ Conceptual Interview Mode
- Text-based answers supported
- Voice-based answers supported
- Speech converted using Whisper
- AI evaluates understanding and clarity

---

### ğŸ”¹ AI Evaluation Metrics
The system provides:
- Technical Score
- Confidence Score
- Improvement Suggestions
- Structured feedback output

---

### ğŸ”¹ Analytics Dashboard
- Interview session history
- Score breakdown per question
- Performance visualization using Chart.js
- Progress tracking

---

### ğŸ” Secure Authentication
- JWT-based authentication
- Encrypted password storage (bcrypt)
- Protected routes
- Environment-based configuration

---

## ğŸ› ï¸ Technology Stack

### Frontend
- React (Vite)
- Redux Toolkit
- Tailwind CSS
- Monaco Editor
- Chart.js

### Backend
- Node.js
- Express.js
- MongoDB
- Mongoose
- JWT Authentication

### AI Microservice
- Python 3.9+
- FastAPI
- Ollama (Mistral LLM)
- OpenAI Whisper
- FFmpeg (audio preprocessing)

---

## ğŸš€ Installation Guide

### ğŸ”§ Prerequisites

Make sure you have:

- Node.js (v16+)
- Python (v3.9+)
- MongoDB (local or Atlas)
- Ollama installed locally
- FFmpeg added to system PATH

---

### ğŸ§  Step 1 â€” Start LLM

```bash
ollama pull mistral
ollama serve

## Step 2 â€” Start AI Microservice

cd ai-service
python -m venv venv
venv\Scripts\activate
pip install fastapi uvicorn ollama openai-whisper pydub python-dotenv python-multipart
uvicorn main:app --reload --port 8000

## Step 3 â€” Start Backend

cd backend
npm install
node server.js

## Step 4 â€” Start Frontend

cd frontend
npm install
npm run dev

## ğŸ” Environment Variables Setup

This project requires environment variables for proper configuration.  
Create a `.env` file inside the respective folders as described below.

---

### ğŸ“ Backend (.env inside `/backend` folder)

Create a file:
backend/.env


Add the following:

```env
PORT=5000
MONGO_URI=mongodb://127.0.0.1:27017/ai_interview_db
JWT_SECRET=your_super_secure_jwt_secret_key
NODE_ENV=development
AI_SERVICE_URL=http://127.0.0.1:8000
ğŸ” Variable Explanation

PORT â†’ Port on which the backend server runs

MONGO_URI â†’ MongoDB connection string (local or Atlas)

JWT_SECRET â†’ Secret key used to sign authentication tokens

NODE_ENV â†’ Environment mode (development / production)

AI_SERVICE_URL â†’ URL of the Python AI microservice

## AI Microservice (.env inside /ai-service folder)

Create:

ai-service/.env

Add:

AI_SERVICE_PORT=8000
OLLAMA_MODEL_NAME=mistral
ğŸ” Variable Explanation

AI_SERVICE_PORT â†’ Port for FastAPI service

OLLAMA_MODEL_NAME â†’ Local LLM model name (default: mistral)

## Frontend (.env inside /frontend folder)

Create:

frontend/.env

Add:

VITE_API_URL=http://localhost:5000/api